# Multiâ€‘Agent Content Generation System (Final Updated README)

## Overview

This project implements a **modular, deterministic, LangChainâ€‘orchestrated content generation system** that converts a raw product JSON into three structured artifacts:

* `product_page.json`
* `comparison_page.json`
* `faq.json`

It satisfies the requirements of the **Applied AI Engineer Challenge**, emphasizing:

* Zero hallucinations
* Deterministic, testable agents
* Fully traceable LangChain pipeline
* Optional LLM augmentation (via Ollama) **only for paraphrasing**, never fact generation
* Clean separation of concerns between parsing, logic, comparison, and templating

The system ships in **two modes**:

### ðŸ”¹ 1. Deterministic Mode (Default)

No LLM calls. All text generation is ruleâ€‘based and reproducible.

### ðŸ”¹ 2. LLMâ€‘Augmented Mode (Optional)

Uses the **safe paraphrasing adapter** to rewrite deterministic answers *without altering facts*.
Enabled via environment variable:

```env
USE_OLLAMA=1
```

---

## Project Structure

```
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ data_parser.py
â”‚   â”œâ”€â”€ question_generator.py
â”‚   â”œâ”€â”€ logic_engine.py
â”‚   â”œâ”€â”€ template_engine.py
â”‚   â”œâ”€â”€ langchain_runnables.py
â”‚   â”œâ”€â”€ langchain_adapters.py
â”‚   â”œâ”€â”€ ollama_adapter.py   # optional paraphrasing
â”œâ”€â”€ logic_blocks/
â”œâ”€â”€ templates/
â”œâ”€â”€ tests/
â”œâ”€â”€ inputs/product_input.json
â”œâ”€â”€ outputs/
â”œâ”€â”€ run_pipeline.py
â””â”€â”€ README.md
```

---

## System Architecture

The system consists of four core agents wrapped as **LangChain Runnables**:

### 1. **DataParserAgent**

Normalizes raw product JSON â†’ typed `ProductModel`.
Handles:

* fuzzy key matching
* price parsing
* validation

### 2. **QuestionGeneratorAgent**

Deterministically produces the required FAQ onboarding questions.

### 3. **LogicBlockEngineAgent**

Executes modular logic blocks located in `logic_blocks/`:

* product_block
* benefits_block
* usage_block
* safety_block
* ingredients_block
* compare_block
* purchase_block
* faq_answer_block (**refined v2** categoryâ€‘aware, safe)

This stage also supports **optional paraphrasing** via the LLM adapter.

### 4. **TemplateEngineAgent**

Renders structured JSON outputs from templates.
Supports:

* dynamic field resolution
* fallback rules
* maxâ€‘length enforcement
* usage â†’ steps transformation

---

## LangChain Runnable DAG

```mermaid
flowchart TD

    A[Raw Product Input]
    B[parse_r - DataParserAgent]
    C[questions_r - QuestionGeneratorAgent]
    D[logic_r - LogicBlockEngineAgent]
    E[template_r - TemplateEngineAgent]

    A --> B --> C --> D --> E

    E --> F[product_page.json]
    E --> G[faq.json]
    E --> H[comparison_page.json]
```

---

## Execution Flow (Highâ€‘Level)

1. **load_input** â†’ product JSON
2. **parse_r.invoke** â†’ structured model
3. **questions_r.invoke** â†’ deterministic FAQ question set
4. **logic_r.invoke** â†’ all logic blocks run, including comparison engine and FAQ v2 answerer
5. **template_r.invoke** â†’ renders JSON outputs
6. **write_outputs** â†’ stored in `outputs/`

Each step is fully logged and traceable.

---

## FAQ Answering Logic (Refined v2)

The FAQ system uses a **6â€‘category taxonomy**:

* `overview`
* `usage`
* `ingredients` (includes compatibility)
* `safety` (includes skinâ€‘type suitability)
* `value` (price + purchase + valueâ€‘forâ€‘money)
* `other` (storage, shelf life, comparison)

### Key Properties

* Zero hallucinations
* Strong question intent matching
* Dedicated compatibility fallbacks
* Deterministic explanations for concentration, suitability, shelfâ€‘life
* Optional LLM paraphrasing with **factual drift detection**

---

## Testing

Run all tests with:

```bash
pytest -q
```

Test coverage includes:

* product parsing
* question generation
* comparison logic (scoring, recommendations)
* FAQ answering (v2 deterministic logic)
* E2E pipeline execution

---

## ðŸ›  Running the Pipeline

```bash
python run_pipeline.py -i inputs/product_input.json -o outputs/
```

Outputs appear in:

```
outputs/product_page.json
outputs/faq.json
outputs/comparison_page.json
```

---

## Mode Switching (Deterministic â†” LLM)

### Deterministic Mode

```env
USE_OLLAMA=0
```

### LLMâ€‘Augmented Mode

```env
USE_OLLAMA=1
OLLAMA_BASE=http://localhost:11434
OLLAMA_MODEL=llama3:8b
```

In LLM mode:

* Only paraphrasing occurs
* All factual content comes from deterministic blocks
* Drift detector ensures paraphrases stay faithful

---

## Design Principles

* **Deterministic by default** (99% of the system)
* **Optional LLM augmentation** for natural phrasing
* **Separation of concerns** via wellâ€‘scoped agents
* **Composable pipeline** using LangChain Runnables
* **Full transparency** â€” all stages logged and traceable
* **Safety first** â€” never generate new facts through LLMs

---

## Extensibility

You can easily:

* Add new logic blocks under `logic_blocks/`
* Extend templates under `templates/`
* Swap LLMs by providing a new paraphrase adapter
* Add new output types (e.g., marketing copy, SEO bullets)

---

## License

For evaluation and demonstration purposes only.

---

For full architecture, execution flow diagrams, and system design documents, refer to the project documentation files in this repository.
